"""
NI√äN LU·∫¨N NGHI√äN C·ª®U
ƒê·ªÅ t√†i: Nghi√™n c·ª©u ph√°t hi·ªán tin gi·∫£ tr√™n m·∫°ng x√£ h·ªôi d·ª±a tr√™n ƒë·∫∑c tr∆∞ng c·∫£m x√∫c k√©p

Dataset: PHEME (Rumour Veracity Classification)
B√†i to√°n: Ph√°t hi·ªán tin gi·∫£ (TRUE vs FALSE)

√ù t∆∞·ªüng ch√≠nh:
- K·∫øt h·ª£p embedding c·ªßa source tweet v·ªõi dual-emotion features
- Tr√≠ch xu·∫•t c·∫£m x√∫c:
    + Source tweet (c·∫£m x√∫c ngu·ªìn)
    + Replies (ph√¢n b·ªë c·∫£m x√∫c ph·∫£n h·ªìi)
- T√≠nh dual-emotion gap v√† th·ªëng k√™ ph√¢n b·ªë replies
- Hu·∫•n luy·ªán v√† so s√°nh 4 m√¥ h√¨nh ML
    1. Logistic Regression
    2. SVM (RBF)
    3. Random Forest
    4. XGBoost
- GridSearchCV ƒë·ªÉ t·ªëi ∆∞u si√™u tham s·ªë
- ƒêo th·ªùi gian hu·∫•n luy·ªán v√† bi·ªÉu ƒë·ªì so s√°nh

"""

# ================== IMPORT ==================
import os
import json
import re
from collections import Counter
from tqdm import tqdm
import numpy as np
import pandas as pd
import warnings
import torch
import time
import matplotlib.pyplot as plt

from transformers import pipeline
from sentence_transformers import SentenceTransformer

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

warnings.filterwarnings("ignore")

# ================== C·∫§U H√åNH ==================
BASE_PATH = os.path.join(os.path.dirname(__file__), "PHEME_veracity")
EMOTION_MODEL = "j-hartmann/emotion-english-distilroberta-base"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
TEST_SIZE = 0.2
RANDOM_STATE = 42
EMOTION_LABELS = ["anger", "disgust", "fear", "joy", "neutral", "sadness", "surprise"]

# ================== TI·ªÄN X·ª¨ L√ù ==================
def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n tweet"""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def is_visible(name):
    """B·ªè qua th∆∞ m·ª•c ·∫©n"""
    return isinstance(name, str) and not name.startswith((".", "_"))

# ================== NH√ÉN VERACITY ==================
def extract_veracity(annotation):
    """0=TRUE, 1=FALSE, None=UNVERIFIED"""
    if not isinstance(annotation, dict):
        return None
    if str(annotation.get("true", "")).strip() == "1":
        return 0
    if str(annotation.get("misinformation", "")).strip() == "1":
        return 1
    return None

# ================== LOAD DATASET ==================
def load_pheme(base_path):
    """ƒê·ªçc to√†n b·ªô dataset PHEME"""
    records = []
    events = [e for e in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, e)) and is_visible(e)]
    print("üìÇ C√°c s·ª± ki·ªán:", events)

    for event in tqdm(events, desc="ƒê·ªçc s·ª± ki·ªán"):
        event_path = os.path.join(base_path, event)
        rumours_path = os.path.join(event_path, "rumours")
        if not os.path.isdir(rumours_path):
            continue
        for thread_id in os.listdir(rumours_path):
            if not is_visible(thread_id):
                continue
            tpath = os.path.join(rumours_path, thread_id)
            ann_path = os.path.join(tpath, "annotation.json")
            if not os.path.exists(ann_path):
                continue
            with open(ann_path, "r", encoding="utf-8") as f:
                ann = json.load(f)
            label = extract_veracity(ann)
            if label is None:
                continue

            # -------- source tweet --------
            source_text = ""
            for root, _, files in os.walk(tpath):
                for fn in files:
                    if fn.endswith(".json") and not fn.startswith(("annotation", "structure")):
                        try:
                            with open(os.path.join(root, fn), "r", encoding="utf-8") as f:
                                j = json.load(f)
                            source_text = j.get("text") or j.get("tweet_text") or ""
                            if source_text:
                                break
                        except:
                            pass
                if source_text:
                    break
            if not source_text:
                continue

            # -------- replies --------
            replies = []
            rdir = os.path.join(tpath, "reactions")
            if os.path.isdir(rdir):
                for fn in os.listdir(rdir):
                    if fn.endswith(".json"):
                        try:
                            with open(os.path.join(rdir, fn), "r", encoding="utf-8") as f:
                                j = json.load(f)
                            txt = j.get("text") or ""
                            if txt:
                                replies.append(txt)
                        except:
                            pass

            records.append({"event": event, "thread_id": thread_id, "source_text": source_text, "replies": replies, "label": label})

    df = pd.DataFrame(records)
    print("üìä Ph√¢n b·ªë nh√£n:", Counter(df["label"]))
    df["source_text_clean"] = df["source_text"].apply(clean_text)
    df["replies_clean"] = df["replies"].apply(lambda L: [clean_text(x) for x in L])
    return df

# ================== EMOTION ==================
def get_emotions(texts, pipe, batch_size=32):
    """D·ª± ƒëo√°n c·∫£m x√∫c theo batch"""
    outputs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        out = pipe(batch, truncation=True, padding=True)
        outputs.extend([o['label'].lower() for o in out])
    return outputs

def reply_distribution(replies, pipe):
    """Ph√¢n b·ªë c·∫£m x√∫c replies + th·ªëng k√™"""
    dist = {e: 0.0 for e in EMOTION_LABELS}
    if not replies:
        dist['neutral'] = 1.0
        return dist
    emos = get_emotions(replies, pipe)
    cnt = Counter(emos)
    total = len(emos)
    for e, c in cnt.items():
        if e in dist:
            dist[e] = c / total
    return dist

# ================== FEATURE ==================
def build_features(src_emo, reply_dist, embeddings):
    """K·∫øt h·ª£p dual-emotion + embedding + th·ªëng k√™ replies"""
    # Source one-hot
    src_df = pd.get_dummies(pd.Series(src_emo), prefix="src")
    for e in EMOTION_LABELS:
        col = f"src_{e}"
        if col not in src_df.columns:
            src_df[col] = 0
    src_df = src_df[[f"src_{e}" for e in EMOTION_LABELS]]

    # Reply distribution + statistics
    rep_df = pd.DataFrame(reply_dist).fillna(0)
    for e in EMOTION_LABELS:
        if e not in rep_df.columns:
            rep_df[e] = 0
    rep_df = rep_df[EMOTION_LABELS]

    # Gap
    gap = src_df.values - rep_df.values

    # Embedding
    X = np.hstack([embeddings, src_df.values, rep_df.values, gap])

    return X

# ================== MAIN ==================
def main():
    print("=== LOAD DATA ===")
    df = load_pheme(BASE_PATH)

    # Device cho pipeline
    device = 0 if torch.cuda.is_available() else -1
    emo_pipe = pipeline("text-classification", model=EMOTION_MODEL, device=device)
    embed_model = SentenceTransformer(EMBEDDING_MODEL, device='cuda' if torch.cuda.is_available() else 'cpu')

    print("=== TR√çCH XU·∫§T C·∫¢M X√öC ===")
    # Source emotions
    src_emo = get_emotions(df["source_text_clean"].tolist(), emo_pipe)
    # Reply distribution
    rep_dist = [reply_distribution(r, emo_pipe) for r in tqdm(df["replies_clean"], desc="Replies")]

    print("=== T√çNH EMBEDDING SOURCE ===")
    embeddings = embed_model.encode(df["source_text_clean"].tolist(), batch_size=32, show_progress_bar=True)

    print("=== BUILD FEATURES ===")
    X = build_features(src_emo, rep_dist, embeddings)
    y = df["label"].values

    # Split v√† SMOTE
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)
    X_tr, y_tr = SMOTE(random_state=RANDOM_STATE).fit_resample(X_tr, y_tr)

    # ================== M√î H√åNH ==================
    models = {
        "LogisticRegression": {"model": LogisticRegression(max_iter=1000), "params": {"clf__C": [0.01, 0.1, 1, 10]}},
        "SVM_RBF": {"model": SVC(kernel="rbf"), "params": {"clf__C": [0.1, 1, 10], "clf__gamma": ["scale", 0.01, 0.001]}},
        "RandomForest": {"model": RandomForestClassifier(), "params": {"clf__n_estimators": [100, 300], "clf__max_depth": [None, 10, 20]}},
        "XGBoost": {"model": XGBClassifier(eval_metric="logloss", use_label_encoder=False), "params": {"clf__n_estimators": [100, 300],
                    "clf__max_depth": [3,6], "clf__learning_rate": [0.01, 0.1]}}
    }

    results = {}

    for name, cfg in models.items():
        print(f"\n===== {name} =====")
        pipe_clf = Pipeline([("scaler", StandardScaler()), ("clf", cfg["model"])])
        grid = GridSearchCV(pipe_clf, cfg["params"], cv=5, scoring="f1", n_jobs=-1)

        start_time = time.time()
        grid.fit(X_tr, y_tr)
        elapsed = time.time() - start_time

        y_pred = grid.best_estimator_.predict(X_te)

        acc = accuracy_score(y_te, y_pred)
        f1 = f1_score(y_te, y_pred)
        rec = recall_score(y_te, y_pred)

        results[name] = {"accuracy": acc, "f1": f1, "recall": rec, "time": elapsed}

        print(f"Time to train {name}: {elapsed:.2f} seconds")
        print("Accuracy:", acc, "F1-score:", f1, "Recall:", rec)
        print("Best params:", grid.best_params_)
        print(classification_report(y_te, y_pred, target_names=["True", "False"]))

    # ================== V·∫º BI·ªÇU ƒê·ªí ==================
    models_list = list(results.keys())
    accuracy = [results[m]["accuracy"] for m in models_list]
    f1_score_list = [results[m]["f1"] for m in models_list]
    recall_list = [results[m]["recall"] for m in models_list]
    time_list = [results[m]["time"] for m in models_list]

    x = np.arange(len(models_list))
    width = 0.2

    # Bi·ªÉu ƒë·ªì Accuracy, F1, Recall
    plt.figure(figsize=(12,6))
    plt.bar(x - width, accuracy, width, label="Accuracy", color='skyblue')
    plt.bar(x, f1_score_list, width, label="F1-score", color='salmon')
    plt.bar(x + width, recall_list, width, label="Recall", color='lightgreen')
    plt.xticks(x, models_list)
    plt.ylim(0,1)
    plt.ylabel("Score")
    plt.title("So s√°nh Accuracy, F1-score, Recall gi·ªØa c√°c m√¥ h√¨nh")
    plt.legend()
    plt.show()

    # Bi·ªÉu ƒë·ªì th·ªùi gian hu·∫•n luy·ªán
    plt.figure(figsize=(10,5))
    plt.bar(models_list, time_list, color='orchid')
    plt.ylabel("Time (seconds)")
    plt.title("So s√°nh th·ªùi gian hu·∫•n luy·ªán c√°c m√¥ h√¨nh")
    plt.show()

if __name__ == "__main__":
    main()